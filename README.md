# LLaMA2
This is the implementation of the paper [Llama 2: Open Foundation and Fine-Tuned Chat Models
](https://arxiv.org/abs/2307.09288). 
The code has been written in Python, so make the necessary changes to the scripts to run them.
Thanks to [Umar Jamilai](https://www.youtube.com/@umarjamilai) and his videos on the topic, they were extremely helpful and valuable.

More comments will be added in the future to make the process of understanding the code easier.

To download the LLaMA weights, go to the download section of [LLaMA repository](https://github.com/facebookresearch/llama/tree/main?tab=readme-ov-file), you will get a url on your email that is required in the *download.sh* file

## Citation:
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., . . . Scialom, T. (2023).
<br>
*Llama 2: Open Foundation and Fine-Tuned Chat Models* 
<br>
[ArXiv.](https://arxiv.org/abs/2307.09288)
